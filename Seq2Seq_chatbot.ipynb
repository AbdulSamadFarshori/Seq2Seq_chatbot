{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7pw7cEfFf5m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Bidirectional, LSTM, Dense, TimeDistributed, Embedding, RepeatVector, Input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from keras.layers import Layer\n",
    "from keras import utils\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.optimizers import RMSprop, adam, Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E646ZsgDGYwX"
   },
   "source": [
    "## import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHPC5k6YGDip"
   },
   "outputs": [],
   "source": [
    "with open('intro_data.txt', 'r') as f:\n",
    "    files = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J3XBNG0xwZsn",
    "outputId": "554f4c94-bf19-488e-be53-a588a5d10b2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = files.split('\\n')\n",
    "len(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRdOZ-v7IK1O"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "separatly store question and reply data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEMYR9TLHVEw"
   },
   "outputs": [],
   "source": [
    "ask = []\n",
    "reply = []\n",
    "for line in con:\n",
    "    _line = line.split('##')\n",
    "    if len(_line) == 2:\n",
    "        ask.append(_line[0])\n",
    "        reply.append(_line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UBPivV-lJuv0",
    "outputId": "4fc73892-49b1-4a14-d49c-37960edb81aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "763"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "j3M_H7K72ewW",
    "outputId": "5ebcad4b-18df-4cba-b822-6fcfeabed9fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "763"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtJxr0RPWDx5"
   },
   "source": [
    "Now cleaning the text make it more readable and reomve unneccessary things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5_M5xBkV09x"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"sheâ€™s\", \"she will\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"why's\", \"why is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"haven't\", \"have not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"aren't\", \"are not\", text)\n",
    "    text = re.sub(r\"isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"didn't\", \"did not\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\" ok \", \" okay \", text)\n",
    "    text = re.sub(r\" thankyou \", \" thank you \", text)\n",
    "    \n",
    "    \n",
    "  \n",
    "\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KFGTgK9V9hz"
   },
   "outputs": [],
   "source": [
    "clean_ask = []\n",
    "for que in ask:\n",
    "    clean_ask.append(clean_text(que))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbuxZhMFqyxk"
   },
   "outputs": [],
   "source": [
    "clean_reply = []\n",
    "for ans in reply:\n",
    "    clean_reply.append(clean_text(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nfPlD-t-Krhm",
    "outputId": "13187fce-353d-42e1-f2af-76aabb27b6b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question hi\n",
      "answer hello\n",
      "question hi\n",
      "answer hello\n",
      "question hi there\n",
      "answer hellothere\n",
      "question hello\n",
      "answer hi there\n",
      "question hi how is it going\n",
      "answer hello fine\n",
      "question hi how are you\n",
      "answer hello i am fine and how are you\n",
      "question hi nice to meet you\n",
      "answer hello nice to meet you too\n",
      "question nice to meet you too\n",
      "answer it is a pleasure to meet you\n",
      "question how are you\n",
      "answer i am good and you\n",
      "question how are you doing\n",
      "answer i am doing well and you\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    print(\"question \" + clean_ask[j])\n",
    "    print(\"answer \" + clean_reply[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SvHD4zM7ztH3",
    "outputId": "c04bcd28-70ab-475d-8ee3-57d39c2985fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763\n",
      "763\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_ask))\n",
    "print(len(clean_reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpFhTzj-YEf0"
   },
   "source": [
    "- create tokenizer and load all words\n",
    "- add SOS and EOS in all sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E5adxxi7ZU_A"
   },
   "outputs": [],
   "source": [
    "reply_tag = []\n",
    "for sen in clean_reply:\n",
    "    final = '<SOS> ' + sen + ' <EOS>'\n",
    "    reply_tag.append(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oZbxCWNbb1u"
   },
   "outputs": [],
   "source": [
    "ask_tag = clean_ask.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tiRWBlBIGKkt",
    "outputId": "de6ef8aa-769a-4b08-d88b-cb742f160204"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', '<SOS> hello <EOS>', 'hi', '<SOS> hello <EOS>', 'hi there']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocabs = []\n",
    "for i in range(len(reply_tag)):\n",
    "    all_vocabs.append(ask_tag[i])\n",
    "    all_vocabs.append(reply_tag[i])\n",
    "all_vocabs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "QECPUc05aY7G",
    "outputId": "907131b5-6d74-4692-cc07-f8b81b792996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889\n"
     ]
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(filters='!')\n",
    "tokenizer.fit_on_texts(all_vocabs)\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8Z7xztzlrm0"
   },
   "source": [
    "prepare data for encoder input and decoder input and decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LUO3umZ4agcw",
    "outputId": "8ee9c048-769a-4da6-8c99-2ef79a61f038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(763, 11) 11\n"
     ]
    }
   ],
   "source": [
    "# encoder input data\n",
    "tokenized_ask = tokenizer.texts_to_sequences(ask_tag) \n",
    "maxlen_ask = max([len(x) for x in tokenized_ask])\n",
    "padded_ask = keras.preprocessing.sequence.pad_sequences(tokenized_ask, maxlen= maxlen_ask,padding='post')\n",
    "encoded_input_data = np.array(padded_ask) \n",
    "print(encoded_input_data.shape, maxlen_ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c90N2qhghUCZ",
    "outputId": "ca2aff94-c216-4191-9801-8d35e589ee38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(763, 19) 19\n"
     ]
    }
   ],
   "source": [
    "# decoder input data\n",
    "tokenized_reply = tokenizer.texts_to_sequences(reply_tag)\n",
    "maxlen_reply = max([len(x) for x in tokenized_reply])\n",
    "padded_reply = keras.preprocessing.sequence.pad_sequences(tokenized_reply,maxlen=maxlen_reply,padding='post')\n",
    "decoded_input_data = np.array(padded_reply)\n",
    "print(decoded_input_data.shape,maxlen_reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TxUTJ64h8Qkc",
    "outputId": "18775dcc-2455-46c2-b976-d5b840a45a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(763, 19, 889)\n"
     ]
    }
   ],
   "source": [
    "# decoder output data\n",
    "tokenized_reply_output = tokenizer.texts_to_sequences(reply_tag)\n",
    "for i in range(len(tokenized_reply_output)):\n",
    "    tokenized_reply_output[i] = tokenized_reply_output[i][1:]\n",
    "pad_decoded_output = keras.preprocessing.sequence.pad_sequences(tokenized_reply_output,maxlen=maxlen_reply,padding='post')\n",
    "#onehot = OneHotEncoder()\n",
    "#decoded_output_data = onehot.fit_transform(pad_decoded_output).toarray()\n",
    "one_hot = utils.to_categorical(pad_decoded_output,vocab_size)\n",
    "decoded_output_data = np.array(one_hot)\n",
    "print(decoded_output_data.shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDJOqwqKimvm"
   },
   "source": [
    "## creating SEQ2SEQ Model\n",
    "\n",
    "\n",
    "<img src=seq2seq.png>\n",
    "\n",
    "\n",
    "- encoder model takes question as the input.\n",
    "- encoder model sates vectors are the initial states for decoder model\n",
    "- for decorder model takes encoder states and starting token as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P45g8ctGUDep"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# encoder cell\n",
    "encoder_input = Input(shape=(None,))\n",
    "encoder_embedded = Embedding(vocab_size,200,mask_zero=True)(encoder_input)\n",
    "encoder_output, state_h, state_c = LSTM(200,return_state=True)(encoder_embedded)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-M_9KZ1NO_w"
   },
   "outputs": [],
   "source": [
    "# decoder cell\n",
    "decoder_input = Input(shape=(None,))\n",
    "decoder_embedded = Embedding(vocab_size, 200, mask_zero=True)(decoder_input)\n",
    "decoder_lstm = LSTM(200, return_state=True,return_sequences= True)\n",
    "decoder_output, _, _ = decoder_lstm(decoder_embedded, initial_state = encoder_states)\n",
    "decoder_dense = Dense(vocab_size,activation='softmax')\n",
    "output = decoder_dense(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u28SkdSlTu4h"
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_input, decoder_input],output)\n",
    "model.compile(optimizer = RMSprop(), loss='categorical_crossentropy',metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "GRNprsgCPLw5",
    "outputId": "bba95e05-feda-4cee-a343-6408ecf975bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    177800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 200)    177800      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200), (None, 320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 200),  320800      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 889)    178689      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,175,889\n",
      "Trainable params: 1,175,889\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H1umLa8KsJL0"
   },
   "source": [
    "Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "S8MHWOnvUjLl",
    "outputId": "fa828ed9-802f-4c39-d0d2-e353990c6bda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/70\n",
      "763/763 [==============================] - 9s 12ms/step - loss: 1.9389 - accuracy: 0.2588\n",
      "Epoch 2/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 1.6574 - accuracy: 0.3065\n",
      "Epoch 3/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 1.5633 - accuracy: 0.3339\n",
      "Epoch 4/70\n",
      "763/763 [==============================] - 9s 11ms/step - loss: 1.4832 - accuracy: 0.3525\n",
      "Epoch 5/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 1.4097 - accuracy: 0.3663\n",
      "Epoch 6/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 1.3457 - accuracy: 0.3816\n",
      "Epoch 7/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 1.2849 - accuracy: 0.3972\n",
      "Epoch 8/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 1.2277 - accuracy: 0.4129\n",
      "Epoch 9/70\n",
      "763/763 [==============================] - 10s 14ms/step - loss: 1.1706 - accuracy: 0.4294\n",
      "Epoch 10/70\n",
      "763/763 [==============================] - 9s 12ms/step - loss: 1.1196 - accuracy: 0.4415\n",
      "Epoch 11/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 1.0680 - accuracy: 0.4616\n",
      "Epoch 12/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 1.0171 - accuracy: 0.4783\n",
      "Epoch 13/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.9683 - accuracy: 0.4921\n",
      "Epoch 14/70\n",
      "763/763 [==============================] - 9s 11ms/step - loss: 0.9231 - accuracy: 0.5122\n",
      "Epoch 15/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.8772 - accuracy: 0.5316\n",
      "Epoch 16/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.8343 - accuracy: 0.5524\n",
      "Epoch 17/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.7905 - accuracy: 0.5704\n",
      "Epoch 18/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.7482 - accuracy: 0.5886\n",
      "Epoch 19/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.7026 - accuracy: 0.6143\n",
      "Epoch 20/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.6640 - accuracy: 0.6292\n",
      "Epoch 21/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.6231 - accuracy: 0.6557\n",
      "Epoch 22/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.5840 - accuracy: 0.6826\n",
      "Epoch 23/70\n",
      "763/763 [==============================] - 9s 12ms/step - loss: 0.5471 - accuracy: 0.7066\n",
      "Epoch 24/70\n",
      "763/763 [==============================] - 9s 11ms/step - loss: 0.5092 - accuracy: 0.7271\n",
      "Epoch 25/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.4736 - accuracy: 0.7533\n",
      "Epoch 26/70\n",
      "763/763 [==============================] - 9s 11ms/step - loss: 0.4387 - accuracy: 0.7734\n",
      "Epoch 27/70\n",
      "763/763 [==============================] - 9s 11ms/step - loss: 0.4077 - accuracy: 0.7979\n",
      "Epoch 28/70\n",
      "763/763 [==============================] - 9s 12ms/step - loss: 0.3750 - accuracy: 0.8192\n",
      "Epoch 29/70\n",
      "763/763 [==============================] - 9s 12ms/step - loss: 0.3438 - accuracy: 0.8423\n",
      "Epoch 30/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.3139 - accuracy: 0.8658\n",
      "Epoch 31/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.2863 - accuracy: 0.8839\n",
      "Epoch 32/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.2597 - accuracy: 0.9007\n",
      "Epoch 33/70\n",
      "763/763 [==============================] - 9s 12ms/step - loss: 0.2352 - accuracy: 0.9138\n",
      "Epoch 34/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.2128 - accuracy: 0.9271\n",
      "Epoch 35/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.1901 - accuracy: 0.9392\n",
      "Epoch 36/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.1719 - accuracy: 0.9470\n",
      "Epoch 37/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.1523 - accuracy: 0.9550\n",
      "Epoch 38/70\n",
      "763/763 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.96 - 8s 10ms/step - loss: 0.1352 - accuracy: 0.9617\n",
      "Epoch 39/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.1211 - accuracy: 0.9687\n",
      "Epoch 40/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.1083 - accuracy: 0.9728\n",
      "Epoch 41/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.0962 - accuracy: 0.9757\n",
      "Epoch 42/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.0847 - accuracy: 0.9775\n",
      "Epoch 43/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.0753 - accuracy: 0.9820\n",
      "Epoch 44/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.0676 - accuracy: 0.9818\n",
      "Epoch 45/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.0592 - accuracy: 0.9852\n",
      "Epoch 46/70\n",
      "763/763 [==============================] - 9s 11ms/step - loss: 0.0529 - accuracy: 0.9867\n",
      "Epoch 47/70\n",
      "763/763 [==============================] - 8s 11ms/step - loss: 0.0470 - accuracy: 0.9862\n",
      "Epoch 48/70\n",
      "763/763 [==============================] - 8s 10ms/step - loss: 0.0414 - accuracy: 0.9877\n",
      "Epoch 49/70\n",
      "763/763 [==============================] - 9s 11ms/step - loss: 0.0377 - accuracy: 0.9884\n",
      "Epoch 50/70\n",
      "763/763 [==============================] - 9s 12ms/step - loss: 0.0338 - accuracy: 0.9891\n",
      "Epoch 51/70\n",
      "763/763 [==============================] - 10s 13ms/step - loss: 0.0305 - accuracy: 0.9894\n",
      "Epoch 52/70\n",
      "763/763 [==============================] - 10s 13ms/step - loss: 0.0275 - accuracy: 0.9905\n",
      "Epoch 53/70\n",
      "763/763 [==============================] - 9s 11ms/step - loss: 0.0255 - accuracy: 0.9894\n",
      "Epoch 54/70\n",
      "763/763 [==============================] - 11s 15ms/step - loss: 0.0225 - accuracy: 0.9908\n",
      "Epoch 55/70\n",
      "763/763 [==============================] - 12s 16ms/step - loss: 0.0212 - accuracy: 0.9906\n",
      "Epoch 56/70\n",
      "763/763 [==============================] - 11s 14ms/step - loss: 0.0190 - accuracy: 0.9905\n",
      "Epoch 57/70\n",
      "763/763 [==============================] - 9s 12ms/step - loss: 0.0180 - accuracy: 0.9905\n",
      "Epoch 58/70\n",
      "763/763 [==============================] - 11s 15ms/step - loss: 0.0167 - accuracy: 0.9922\n",
      "Epoch 59/70\n",
      "763/763 [==============================] - 17s 22ms/step - loss: 0.0161 - accuracy: 0.9905\n",
      "Epoch 60/70\n",
      "763/763 [==============================] - 12s 16ms/step - loss: 0.0142 - accuracy: 0.9917\n",
      "Epoch 61/70\n",
      "763/763 [==============================] - 17s 22ms/step - loss: 0.0136 - accuracy: 0.9910\n",
      "Epoch 62/70\n",
      "763/763 [==============================] - 11s 14ms/step - loss: 0.0138 - accuracy: 0.9917\n",
      "Epoch 63/70\n",
      "763/763 [==============================] - 10s 14ms/step - loss: 0.0121 - accuracy: 0.9917\n",
      "Epoch 64/70\n",
      "763/763 [==============================] - 17s 22ms/step - loss: 0.0121 - accuracy: 0.9920\n",
      "Epoch 65/70\n",
      "763/763 [==============================] - 12s 16ms/step - loss: 0.0108 - accuracy: 0.9918\n",
      "Epoch 66/70\n",
      "763/763 [==============================] - 14s 18ms/step - loss: 0.0107 - accuracy: 0.9911\n",
      "Epoch 67/70\n",
      "763/763 [==============================] - 10s 13ms/step - loss: 0.0107 - accuracy: 0.9905\n",
      "Epoch 68/70\n",
      "763/763 [==============================] - 10s 13ms/step - loss: 0.0103 - accuracy: 0.9917\n",
      "Epoch 69/70\n",
      "763/763 [==============================] - 10s 13ms/step - loss: 0.0097 - accuracy: 0.9923\n",
      "Epoch 70/70\n",
      "763/763 [==============================] - 9s 12ms/step - loss: 0.0093 - accuracy: 0.9918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25f7e34fe10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoded_input_data,decoded_input_data],decoded_output_data,batch_size=10,epochs=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46CY8060ASla"
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdHhkjM42UA1"
   },
   "source": [
    "preparing inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h489V8y_PVcW"
   },
   "outputs": [],
   "source": [
    "def make_inference():\n",
    "    encoder_model = keras.models.Model(encoder_input,encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(200,))\n",
    "    decoder_state_input_c = Input(shape=(200,))\n",
    "\n",
    "    decoder_state_input = [decoder_state_input_h,decoder_state_input_c]\n",
    "\n",
    "    decoder_outputs, state_h,state_c = decoder_lstm(decoder_embedded,initial_state=decoder_state_input)\n",
    "    decoder_states = [state_h,state_c] \n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = keras.models.Model([decoder_input]+decoder_state_input,\n",
    "                                     [decoder_outputs]+decoder_states)\n",
    "  \n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BvlAnO3VU4SG"
   },
   "source": [
    "prepare chatbot for giving final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WzHWg_VSsZj"
   },
   "outputs": [],
   "source": [
    "def str_to_token(sentence:str):\n",
    "    words = sentence.lower().split()\n",
    "    token_list = []\n",
    "    for word in words:\n",
    "        token_list.append(tokenizer.word_index[word])\n",
    "    return keras.preprocessing.sequence.pad_sequences([token_list], maxlen=maxlen_ask, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JctyLw-uX-T5",
    "outputId": "2d88b55a-2f13-452d-a8b2-971540a2fe1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you: hello\n",
      "chatbot: hi there <eos>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc_model, dec_model = make_inference()\n",
    "\n",
    "\n",
    "for _ in range(1):\n",
    "    ask=''\n",
    "    input_user=input('you: ')\n",
    "    temp = input_user\n",
    "    split_input=input_user.split(' ')\n",
    "    for i in range(len(split_input)):\n",
    "        if split_input[i] not in tokenizer.word_index:\n",
    "            unk_word=split_input[i]\n",
    "            split_input[i]= 'ukn'\n",
    "        ask +=split_input[i]+' '\n",
    "    state_value = enc_model.predict(str_to_token(ask))\n",
    "    empty_target_seq = np.zeros((1,1))\n",
    "    empty_target_seq[0,0] = tokenizer.word_index['<sos>']\n",
    "    stop_condition =False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition:\n",
    "        dec_output, h, c = dec_model.predict([empty_target_seq]+state_value)\n",
    "        sample_word_index = np.argmax(dec_output[0,-1,:])\n",
    "        ##print(dec_output[0,-1,:])\n",
    "        sample_word = None\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if sample_word_index == index:\n",
    "                decoded_translation += ' {}' .format(word)\n",
    "                sample_word = word\n",
    "          \n",
    "        if sample_word == '<eos>' or len(decoded_translation.split())> maxlen_reply:\n",
    "            stop_condition = True\n",
    "\n",
    "        empty_target_seq =np.zeros((1,1))\n",
    "        empty_target_seq[0,0] = sample_word_index\n",
    "        state_value = [h,c]\n",
    "    print('chatbot:{}'.format(decoded_translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
